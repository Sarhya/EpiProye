# -*- coding: utf-8 -*-
"""RedesNn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WSdVkk4_8tJ_ity1BbCj6MBGjYHjFkQo
"""

import numpy as np
from scipy.integrate import odeint
import matplotlib.pyplot as plt
import torch
from torch.autograd import grad
import torch.nn as nn
from numpy import genfromtxt
import torch.optim as optim
import matplotlib.pyplot as plt
import torch.nn.functional as F

torch.manual_seed(1234)

def deriv(y, t, be, mu, ga,de):
    S,I,D,R = y
    dS = -be*S*I-mu*S
    dI = be*I*S - I*(mu+ga+de)
    dY = be*I*S
    dD = de*I
    dR = ga*I-mu*R

    return dS, dI, dD, dR

#Generaremos los datos para entrenar a nuesta red
# Condiciones iniciales
N = 20000

S0 = N - 20 
I0 = 20 
D0 = 0
R0 = 0


t = np.linspace(0, 7, 100) #Generamos una semana

#Los datos de entrenamiento fueron los conseguidos por bayesiana.
be=0.690662851033162 
mu=2.2919745500369073
ga=0.172107913995505
de=0.8269732753091987

# Condiciones iniciaÃ±
y0 = S0, I0, D0, R0
#Reticuals
ret = odeint(deriv, y0, t, args=(be, mu, ga,de))
S, I, D, R = ret.T

fig = plt.figure(figsize=(12,12))
ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)
ax.set_facecolor('xkcd:white')

ax.plot(t, S, 'violet', alpha=0.5, lw=2, label='Susceptible', linestyle='dashed')
ax.plot(t, I, 'darkgreen', alpha=0.5, lw=2, label='Infected', linestyle='dashed')
ax.plot(t, D, 'blue', alpha=0.5, lw=2, label='Dead', linestyle='dashed')
ax.plot(t, R, 'red', alpha=0.5, lw=2, label='Recovered', linestyle='dashed')

ax.set_xlabel('Dias')
ax.set_ylabel('Personas')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='black', lw=0.2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.show()

#Guardamos los datos de entrenamiento
datosE = np.asarray([t, S, I, D, R]) 
np.savetxt("datosE.csv", datosE, delimiter=",")

datos = genfromtxt('datosE.csv', delimiter=',') #Descargamos los datos

def train(self, n_epochs):
      # Funcion de entrenamiento
      print('\nstarting training...\n')
      
      for epoch in range(n_epochs):
        #Listas para las salidas, solo se matendra en la ultima epoca
        S_pred_list = []
        I_pred_list = []
        D_pred_list = []
        R_pred_list = []

        # Pasamos los grupos a las redes
        f1, f2, f3, f4, S_pred, I_pred, D_pred, R_pred = self.net_f(self.t_batch) # Salidas de la redf
        
        self.optimizer.zero_grad() #zero grad
        
        #anexamos cada salida (con el fin de graficar)
        S_pred_list.append(self.S_min + (self.S_max - self.S_min) * S_pred)
        I_pred_list.append(self.I_min + (self.I_max - self.I_min) * I_pred)
        D_pred_list.append(self.D_min + (self.D_max - self.D_min) * D_pred)
        R_pred_list.append(self.R_min + (self.R_max - self.R_min) * R_pred)

        #FUncion de perdida
        loss = (torch.mean(torch.square(self.S_hat - S_pred))+ 
                torch.mean(torch.square(self.I_hat - I_pred))+
                torch.mean(torch.square(self.D_hat - D_pred))+
                torch.mean(torch.square(self.R_hat - R_pred))+
                torch.mean(torch.square(f1))+
                torch.mean(torch.square(f2))+
                torch.mean(torch.square(f3))+
                torch.mean(torch.square(f4))
                ) 

        loss.backward()
        self.optimizer.step()
        self.scheduler.step() 

        #Anexamos la funcion de perdidad
        self.losses.append(loss.item())

        if epoch % 1000 == 0:          
          print('\nEpoch ', epoch)

          print('alpha: (goal 0.191 ', self.alpha)
          print('beta: (goal 0.05 ', self.beta)
          print('gamma: (goal 0.0294 ', self.gamma)

          print('#################################')                

      return S_pred_list, I_pred_list, D_pred_list, R_pred_list#>>>>
class DINN(nn.Module):
    def __init__(self, t, S_data, I_data, D_data, R_data): #[t,S,I,D,R]
        super(DINN, self).__init__()
        
        self.N =20000
        
        #Convertimos los datos en tenseroes
        self.t = torch.tensor(t, requires_grad=True)
        self.t_float = self.t.float()
        self.t_batch = torch.reshape(self.t_float, (len(self.t),1)) #reshape for batch 

        #Cada compartimiento en ttensores
        self.S = torch.tensor(S_data)
        self.I = torch.tensor(I_data)
        self.D = torch.tensor(D_data)
        self.R = torch.tensor(R_data)

        self.losses = [] #Guardamos la perdida por epoca

        #Parametrs
        self.beta_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))
        self.mu_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))
        self.gamma_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))
        self.delta_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))

        #Variables para normalizacion
        self.S_max = max(self.S)
        self.I_max = max(self.I)
        self.D_max = max(self.D)
        self.R_max = max(self.R)
        self.S_min = min(self.S)
        self.I_min = min(self.I)
        self.D_min = min(self.D)
        self.R_min = min(self.R)

        #normalizacion
        self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)
        self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)
        self.D_hat = (self.D - self.D_min) / (self.D_max - self.D_min)
        self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)        

        #MAtrices para el jacobiano (4x4)
        self.m1 = torch.zeros((len(self.t), 4)); self.m1[:, 0] = 1
        self.m2 = torch.zeros((len(self.t), 4)); self.m2[:, 1] = 1
        self.m3 = torch.zeros((len(self.t), 4)); self.m3[:, 2] = 1
        self.m4 = torch.zeros((len(self.t), 4)); self.m4[:, 3] = 1

        #NN
        self.net_sidr = self.Net_sidr()
        self.params = list(self.net_sidr.parameters())
        self.params.extend(list([self.beta_tilda, self.mu_tilda, self.gamma_tilda,self.delta_tilda]))

    #Seleccionamos cada paramtro para estar encierto ranfo
    @property
    def beta(self):
        return torch.tanh(self.beta_tilda) #* 0.1 + 0.2

    @property
    def mu(self):
        return torch.tanh(self.mu_tilda) #* 0.01 + 0.05
    
    @property
    def gamma(self):
        return torch.tanh(self.gamma_tilda) #* 0.01 + 0.03

    @property
    def delta(self):
        return torch.tanh(self.delta_tilda) #* 0.01 + 0.03

    class Net_sidr(nn.Module): # entradas = [[t1], [t2]...[t100]] -- tiempos
        def __init__(self):
            super(DINN.Net_sidr, self).__init__()

            self.fc1=nn.Linear(1, 20) 
            self.fc2=nn.Linear(20, 20)
            self.fc3=nn.Linear(20, 20)
            self.fc4=nn.Linear(20, 20)
            self.fc5=nn.Linear(20, 20)
            self.fc6=nn.Linear(20, 20)
            self.fc7=nn.Linear(20, 20)
            self.fc8=nn.Linear(20, 20)
            self.out=nn.Linear(20, 4) #devuelve S,I,D,R

        def forward(self, t_batch):
            sidr=F.relu(self.fc1(t_batch))
            sidr=F.relu(self.fc2(sidr))
            sidr=F.relu(self.fc3(sidr))
            sidr=F.relu(self.fc4(sidr))
            sidr=F.relu(self.fc5(sidr))
            sidr=F.relu(self.fc6(sidr))
            sidr=F.relu(self.fc7(sidr))
            sidr=F.relu(self.fc8(sidr))
            sidr=self.out(sidr)
            return sidr

    def net_f(self, t_batch):
            
            #pASAMOS el tiempo a la red
            sidr_hat = self.net_sidr(t_batch)
            
            #Organizamos el SIDR PARA CADA SALIDA
            S_hat, I_hat, D_hat, R_hat = sidr_hat[:,0], sidr_hat[:,1], sidr_hat[:,2], sidr_hat[:,3]

            #S_t
            sidr_hat.backward(self.m1, retain_graph=True)
            S_hat_t = self.t.grad.clone()
            self.t.grad.zero_()

            #I_t
            sidr_hat.backward(self.m2, retain_graph=True)
            I_hat_t = self.t.grad.clone()
            self.t.grad.zero_()

            #D_t
            sidr_hat.backward(self.m3, retain_graph=True)
            D_hat_t = self.t.grad.clone()
            self.t.grad.zero_()

            #R_t
            sidr_hat.backward(self.m4, retain_graph=True)
            R_hat_t = self.t.grad.clone()
            self.t.grad.zero_() 

            #Desnormalizamos
            S = self.S_min + (self.S_max - self.S_min) * S_hat
            I = self.I_min + (self.I_max - self.I_min) * I_hat
            D = self.D_min + (self.D_max - self.D_min) * D_hat      
            R = self.R_min + (self.R_max - self.R_min) * R_hat        

            f1_hat = S_hat_t - (-self.beta * S * I -self.mu*S)  / (self.S_max - self.S_min)
            f2_hat = I_hat_t - (self.beta * S * I - self.mu * I - self.gamma * I -self.delta*I) / (self.I_max - self.I_min)
            f3_hat = D_hat_t - (self.delta * I) / (self.D_max - self.D_min)
            f4_hat = R_hat_t - (self.gamma * I -self.mu*R) / (self.R_max - self.R_min)        

            return f1_hat, f2_hat, f3_hat, f4_hat, S_hat, I_hat, D_hat, R_hat

    def train(self, n_epochs):
        # FUncion de entrenamiento
        print('\nstarting training...\n')
        
        for epoch in range(n_epochs):
            # lists to hold the output (maintain only the final epoch)
            S_pred_list = []
            I_pred_list = []
            D_pred_list = []
            R_pred_list = []

            # we pass the timesteps batch into net_f
            f1, f2, f3, f4, S_pred, I_pred, D_pred, R_pred = self.net_f(self.t_batch) # net_f outputs f1_hat, f2_hat, f3_hat, f4_hat, S_hat, I_hat, D_hat, R_hat
            
            self.optimizer.zero_grad() #zero grad
            
            #append the values to plot later (note that we unnormalize them here for plotting)
            S_pred_list.append(self.S_min + (self.S_max - self.S_min) * S_pred)
            I_pred_list.append(self.I_min + (self.I_max - self.I_min) * I_pred)
            D_pred_list.append(self.D_min + (self.D_max - self.D_min) * D_pred)
            R_pred_list.append(self.R_min + (self.R_max - self.R_min) * R_pred)

            #calculate the loss --- MSE of the neural networks output and each compartment
            loss = (torch.mean(torch.square(self.S_hat - S_pred))+ 
                    torch.mean(torch.square(self.I_hat - I_pred))+
                    torch.mean(torch.square(self.D_hat - D_pred))+
                    torch.mean(torch.square(self.R_hat - R_pred))+
                    torch.mean(torch.square(f1))+
                    torch.mean(torch.square(f2))+
                    torch.mean(torch.square(f3))+
                    torch.mean(torch.square(f4))
                    ) 

            loss.backward()
            self.optimizer.step()
            self.scheduler.step() 

            # append the loss value (we call "loss.item()" because we just want the value of the loss and not the entire computational graph)
            self.losses.append(loss.item())

            if epoch % 1000 == 0:          
                print('\nEpoch ', epoch)

                print('beta: (goal 0.191 ', self.beta)
                print('mu: (goal 0.05 ', self.mu)
                print('gamma: (goal 0.0294 ', self.gamma)
                print('delta: (goal 0.0294 ', self.delta)

                print('#################################')                

        return S_pred_list, I_pred_list, D_pred_list, R_pred_list

# Commented out IPython magic to ensure Python compatibility.
# %%time
# dinn = DINN(datos[0], datos[1], datos[2], datos[3], datos[4]) #[t,S,I,D,R]
# learning_rate = 1e-6
# optimizer = optim.Adam(dinn.params, lr = learning_rate)
# dinn.optimizer = optimizer
# 
# scheduler = torch.optim.lr_scheduler.CyclicLR(dinn.optimizer, base_lr=1e-5, max_lr=1e-3, step_size_up=1000, mode="exp_range", gamma=0.85, cycle_momentum=False)
# 
# dinn.scheduler = scheduler
# 
# S_pred_list, I_pred_list, D_pred_list, R_pred_list = dinn.train(50000)

#Graficamos las perdidas por epoca
plt.plot(dinn.losses[0:], color = 'teal')
plt.xlabel('Epoca')
plt.ylabel('Perdida')

fig = plt.figure(figsize=(12,12))
ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)
ax.set_facecolor('xkcd:white')

ax.plot(datos[0], datos[1], 'pink', alpha=0.5, lw=2, label='S')
ax.plot(datos[0], S_pred_list[0].detach().numpy(), 'red', alpha=0.9, lw=2, label='Prediccion S', linestyle='dashed')

ax.plot(datos[0], datos[2], 'violet', alpha=0.5, lw=2, label='I')
ax.plot(datos[0], I_pred_list[0].detach().numpy(), 'dodgerblue', alpha=0.9, lw=2, label='Prediccion I', linestyle='dashed')

ax.plot(datos[0], datos[3], 'darkgreen', alpha=0.5, lw=2, label='D')
ax.plot(datos[0], D_pred_list[0].detach().numpy(), 'green', alpha=0.9, lw=2, label='Prediccion D', linestyle='dashed')

ax.plot(datos[0], datos[4], 'blue', alpha=0.5, lw=2, label='R')
ax.plot(datos[0], R_pred_list[0].detach().numpy(), 'teal', alpha=0.9, lw=2, label='Prediccion R', linestyle='dashed')

ax.set_xlabel('Dias')
ax.set_ylabel('Personas')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='black', lw=0.2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.show()